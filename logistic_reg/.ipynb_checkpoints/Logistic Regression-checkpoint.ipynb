{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import e\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Line():\n",
    "    def __init__(self, p):\n",
    "        self.p1 = p[0]\n",
    "        self.p2 = p[1]\n",
    "        self.slope = (self.p1[1]-self.p2[1])/(self.p1[0]-self.p2[0])\n",
    "        self.b = self.p1[1]-self.slope*self.p1[0]\n",
    "        super().__init__()\n",
    "    def calculate(self,x):\n",
    "        return self.slope*x+self.b\n",
    "    def plot(self):\n",
    "        x = np.random.uniform(-1,1,10)\n",
    "        plt.plot(x,self.calculate(x))\n",
    "    def find_actual_y(self,points):\n",
    "        return [np.sign(self.calculate(p[0]) - p[1]) for p in points]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class LogisticRegression(Line):\n",
    "    def __init__(self,N):\n",
    "        self.weights = np.array([0,0,0])\n",
    "        self.n = .01\n",
    "        self.N = N\n",
    "        \n",
    "        self.create_line()\n",
    "        \n",
    "        X = [np.random.uniform(-1.0,1.0,2) for x in range(self.N)]\n",
    "        self.y = self.find_actual_y(X)\n",
    "        self.X = np.concatenate([[[1]for x in range(len(X))],X],axis=1)\n",
    "        \n",
    "        #self.run_sgd()\n",
    "        self.run_gradient_descent()\n",
    "        self.eout()\n",
    "        \n",
    "    def create_line(self):\n",
    "        p = [np.random.uniform(-1.0,1.0,2) for x in range(2)]\n",
    "        while p[0][0] == p[1][0] and p[0][1] == p[1][1]:\n",
    "            p = [np.random.uniform(-1.0,1.0,2) for x in range(2)]\n",
    "        super().__init__(p)\n",
    "  \n",
    "    def run_sgd(self):\n",
    "        self.num_iter = 0\n",
    "        cur_wdiff = 1\n",
    "        while True:\n",
    "            self.num_iter += 1\n",
    "            wt = self.weights\n",
    "            self.sgd_epoch()\n",
    "            wt1 = self.weights\n",
    "            if np.sqrt(sum((wt-wt1)**2)) <.01: #np.linalg.norm\n",
    "                break\n",
    "            \n",
    "        self.ein = self.cross_entropy_error(self.X,self.y)\n",
    "    \n",
    "    \n",
    "    def run_gradient_descent(self):\n",
    "        self.num_iter = 0\n",
    "        \n",
    "        for x in range(1000):\n",
    "            self.num_iter += 1\n",
    "            wt = self.weights\n",
    "            self.gradient_descent()\n",
    "            wt1 = self.weights\n",
    "            \n",
    "        \n",
    "        self.ein = self.cross_entropy_error(self.X,self.y)\n",
    "         \n",
    "        \n",
    "        \n",
    "####################################################\n",
    "        \n",
    "    def sdg_gradient(self,x,y):\n",
    "        w = self.weights\n",
    "        return (-y*x/(1+e**(y*np.matmul(x,w))))\n",
    "    \n",
    "    \n",
    "    def gradient(self):\n",
    "        w = self.weights\n",
    "        return sum([(-self.y[v]*self.X[v]/(1+e**(self.y[v]*np.matmul(self.X[v],w)))) for v in range(self.N)])/self.N #-derivative of Ein (or Eug if regularized)\n",
    "         \n",
    "    \n",
    "####################################################\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        gradient = -self.n *self.gradient()\n",
    "        self.weights = self.weights + gradient \n",
    "        \n",
    "    def sgd_epoch(self):\n",
    "        vals = list(range(self.N))\n",
    "        random.shuffle(vals)\n",
    "        for v in vals:\n",
    "            gradient = -self.n *self.sdg_gradient(self.X[v],self.y[v])\n",
    "            self.weights = self.weights + gradient  \n",
    "    \n",
    "    \n",
    "######################################################\n",
    "    \n",
    "    def cross_entropy_error(self,X,y):\n",
    "        w = self.weights\n",
    "        return sum([math.log(1+e**(-y[v]*np.matmul(X[v],w)), e) for v in range(len(X))])/len(X)\n",
    "    def calc(self,X):\n",
    "        return 1/(1+e**-np.matmul(X,self.weights))\n",
    "    \n",
    "    def eout(self):\n",
    "        X = [np.random.uniform(-1.0,1.0,2) for x in range(100)]\n",
    "        X = np.concatenate([[[1]for x in range(len(X))],X],axis=1)\n",
    "        y = self.find_actual_y(X)\n",
    "        self.eout = self.cross_entropy_error(X,y)\n",
    "        print(self.calc(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_experiment():\n",
    "    def __init__(self,N, numEx):\n",
    "        self.N = N\n",
    "        self.numEx = numEx\n",
    "        self.run() \n",
    "    def run(self):\n",
    "        eouts = []\n",
    "        eins = []\n",
    "        num_iters = []\n",
    "        for x in range(self.numEx):\n",
    "            test = LogisticRegression(self.N)\n",
    "            eouts.append(test.eout)\n",
    "            eins.append(test.ein)\n",
    "            num_iters.append(test.num_iter)\n",
    "        Eout = np.mean(eouts)\n",
    "        NumIter = np.mean(num_iters)\n",
    "        Ein = np.mean(eins)\n",
    "        print(\"Eout Error %s\" % Eout)\n",
    "        print(\"Ein Error %s\" % Ein)\n",
    "        print(\"Num Iterations %s\" % NumIter) \n",
    "           \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eout Error 0.8903865048180757\n",
      "Ein Error 0.3465688744034626\n",
      "Num Iterations 1000.0\n"
     ]
    }
   ],
   "source": [
    "obj = run_experiment(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14517135 0.3511087  0.20980148 0.34260044 0.19041879 0.20139264\n",
      " 0.48063508 0.23202239 0.14359317 0.16479985 0.15798298 0.22791657\n",
      " 0.23595653 0.35497404 0.35086482 0.27166552 0.09772059 0.34853312\n",
      " 0.26892184 0.22382384 0.17874718 0.10549682 0.11626896 0.20706962\n",
      " 0.29298509 0.11836746 0.1431996  0.2162219  0.3170426  0.26943624\n",
      " 0.47036953 0.48199806 0.28951466 0.43096578 0.29685679 0.2428104\n",
      " 0.17378495 0.20100609 0.11156849 0.29915913 0.16138042 0.18641235\n",
      " 0.3011242  0.45573347 0.11261919 0.29692581 0.28968493 0.11327043\n",
      " 0.33057544 0.08386531 0.38464909 0.42774045 0.27529055 0.2287523\n",
      " 0.40160042 0.20617206 0.24348096 0.25624469 0.32145334 0.2184356\n",
      " 0.208543   0.16892408 0.19723572 0.1842127  0.20768977 0.18346604\n",
      " 0.13902438 0.31798656 0.19603937 0.10306261 0.15367669 0.23018049\n",
      " 0.1983501  0.31772603 0.31792    0.38295793 0.31573774 0.125657\n",
      " 0.16310838 0.16284687 0.11602391 0.16098525 0.24450672 0.30001274\n",
      " 0.25748028 0.08860808 0.1907304  0.20356567 0.13936108 0.25105803\n",
      " 0.24786065 0.27678544 0.13615758 0.51265506 0.28475602 0.34564917\n",
      " 0.26593885 0.33960725 0.15621984 0.09015731]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegression at 0x7fdb30128ee0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.shuffle([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_idxs = np.arange(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7320508075688772"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
